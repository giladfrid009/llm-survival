{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the lightning model from checkpoint\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.datasets import PropDataset\n",
    "\n",
    "# ds_train = PropDataset(\"data/rtp_500/split_1_0.5_0.1_0.2_0.2/cal.pkl\", score_name=\"toxicity\", threshold=0.5)\n",
    "# ds_test = PropDataset(\"data/rtp_500/split_1_0.5_0.1_0.2_0.2/test.pkl\", score_name=\"toxicity\", threshold=0.5)\n",
    "# ds_val = PropDataset(\"data/rtp_500/split_1_0.5_0.1_0.2_0.2/val.pkl\", score_name=\"toxicity\", threshold=0.5)\n",
    "# ds_cal = PropDataset(\"data/rtp_500/split_1_0.5_0.1_0.2_0.2/cal.pkl\", score_name=\"toxicity\", threshold=0.5)\n",
    "\n",
    "# # extract (x, y) samples from these datasets, and save them into lists\n",
    "# # each list corresponds to a different dataset\n",
    "\n",
    "# def extract_samples(ds):\n",
    "#     data = []\n",
    "#     for i in tqdm(range(len(ds))):\n",
    "#         x, y = ds[i]\n",
    "#         data.append((x, y[0]))\n",
    "#     return data\n",
    "\n",
    "# train_data = extract_samples(ds_train)\n",
    "# val_data = extract_samples(ds_val)\n",
    "# test_data = extract_samples(ds_test)\n",
    "# cal_data = extract_samples(ds_cal)\n",
    "\n",
    "# # save lists using pickle\n",
    "\n",
    "# import pickle\n",
    "\n",
    "# with open(\"data/rtp_500/prob_data/cal_data.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(cal_data, f)\n",
    "\n",
    "# with open(\"data/rtp_500/prob_data/train_data.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(train_data, f)\n",
    "\n",
    "# with open(\"data/rtp_500/prob_data/val_data.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(val_data, f)\n",
    "\n",
    "# with open(\"data/rtp_500/prob_data/test_data.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(test_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "class ProbDataset(Dataset):\n",
    "    def __init__(self, data_path, binary: bool = True, prob_thresh: float = 1 / 500):\n",
    "        with open(data_path, \"rb\") as f:\n",
    "            self.data = pickle.load(f)\n",
    "        self.binary = binary\n",
    "        self.prob_thresh = prob_thresh\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.data[idx]\n",
    "        \n",
    "        if self.binary:\n",
    "            # convert to binary\n",
    "            y = 0.0 if y <= self.prob_thresh else 1.0\n",
    "            \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = ProbDataset(\"data/rtp_500/prob_data/cal_data.pkl\")\n",
    "ds_test = ProbDataset(\"data/rtp_500/prob_data/test_data.pkl\")\n",
    "ds_val = ProbDataset(\"data/rtp_500/prob_data/val_data.pkl\")\n",
    "ds_cal = ProbDataset(\"data/rtp_500/prob_data/cal_data.pkl\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = DataLoader(ds_train, batch_size=256, shuffle=True)\n",
    "dl_val = DataLoader(ds_val, batch_size=256, shuffle=False)\n",
    "dl_test = DataLoader(ds_test, batch_size=256, shuffle=False)\n",
    "dl_cal = DataLoader(ds_cal, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "class LLMModel(nn.Module):\n",
    "    def __init__(self, model: nn.Module, tokenizer, device: str = \"cuda\"):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        inputs = self.tokenizer(x, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        inputs = inputs.to(self.device)\n",
    "        logits = self.model(**inputs)[0][:, 0]\n",
    "        return logits\n",
    "\n",
    "\n",
    "class ToxicScorer(pl.LightningModule):\n",
    "    def __init__(self, model: nn.Module, optimizer: torch.optim.Optimizer, criterion: nn.Module, device: str = \"cuda\"):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "\n",
    "        self.model.to(self.device)\n",
    "        self.criterion.to(self.device)\n",
    "        self.to(device)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self.optimizer\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "\n",
    "        probs = torch.sigmoid(y_hat)\n",
    "        accuracy = ((y > 0.5) == (probs > 0.5)).float().mean()\n",
    "        self.log(\"val_accuracy\", accuracy, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self.forward(x)\n",
    "        return torch.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.failure_model import ToxicClassifier\n",
    "\n",
    "model_path = \"saved/Jigsaw_BERT/lightning_logs/version_0/checkpoints/epoch=7-step=1552.ckpt\"\n",
    "failure_model = ToxicClassifier.load_from_checkpoint(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = LLMModel(failure_model.model, failure_model.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer_model = ToxicScorer(\n",
    "    model=llm_model,\n",
    "    optimizer=torch.optim.Adam(llm_model.parameters(), lr=1e-5),\n",
    "    criterion=nn.BCEWithLogitsLoss(),\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "_ = scorer_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    monitor=\"val_accuracy\",\n",
    "    mode=\"max\",\n",
    "    save_top_k=1,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    max_epochs=20,\n",
    "    enable_checkpointing=True,\n",
    "    logger=True,\n",
    "    callbacks=[checkpoint],\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    scorer_model,\n",
    "    train_dataloaders=dl_train,\n",
    "    val_dataloaders=dl_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# get all model predictions and GT labels\n",
    "\n",
    "preds = trainer.predict(model=scorer_model, dataloaders=dl_cal)\n",
    "\n",
    "preds = torch.cat(preds)\n",
    "\n",
    "labels = []\n",
    "for _, y in tqdm(ds_cal):\n",
    "    labels.append(y)\n",
    "\n",
    "labels = torch.tensor(labels).to(preds.device)\n",
    "\n",
    "overall_accuracy = ((labels > 0.5) == (preds > 0.5)).float().mean()\n",
    "print(f\"Overall accuracy: {overall_accuracy.item() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = torch.sort(preds, descending=False)[0]\n",
    "thresholds = thresholds.to(preds.device)\n",
    "\n",
    "pos_accs = []  # accuracy for samples with pred >= threshold\n",
    "neg_accs = []  # accuracy for samples with pred <= threshold\n",
    "\n",
    "for threshold in tqdm(thresholds):\n",
    "    pos_mask = preds >= threshold\n",
    "    neg_mask = preds <= threshold\n",
    "\n",
    "    # pos_acc = torch.where(preds[pos_mask] > 0.5, labels[pos_mask], 1.0 - labels[pos_mask]).float().mean()\n",
    "    # neg_acc = torch.where(preds[neg_mask] < 0.5, 1.0 - labels[neg_mask], labels[neg_mask]).float().mean()\n",
    "\n",
    "    pos_acc = ((labels[pos_mask] > 0.5) == (preds[pos_mask] > 0.5)).float().mean()\n",
    "    neg_acc = ((labels[neg_mask] < 0.5) == (preds[neg_mask] < 0.5)).float().mean()\n",
    "\n",
    "    pos_accs.append(pos_acc)\n",
    "    neg_accs.append(neg_acc)\n",
    "\n",
    "pos_accs = torch.stack(pos_accs)\n",
    "neg_accs = torch.stack(neg_accs)\n",
    "\n",
    "pos_counts = []  # number of items with pred >= threshold\n",
    "neg_counts = []  # number of items with pred <= threshold\n",
    "\n",
    "for threshold in tqdm(thresholds):\n",
    "    pos_mask = preds >= threshold\n",
    "    neg_mask = preds <= threshold\n",
    "\n",
    "    pos_counts.append(pos_mask.sum())\n",
    "    neg_counts.append(neg_mask.sum())\n",
    "\n",
    "pos_counts = torch.stack(pos_counts)\n",
    "neg_counts = torch.stack(neg_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hoeff_expected(\n",
    "    accs: torch.Tensor,\n",
    "    counts: torch.Tensor,\n",
    "    target_accuracy: float,\n",
    "    order: int = 2,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute Expected value based on Hoeffding series for positive and negative accuracies.\n",
    "    for delta = |acc - target_accuracy| / 2, Hoeffding series is constructed by estimating\n",
    "    Pr(acc - target_accuracy < delta * i) for i = 1, 2, ... by using Hoeffding's inequality.\n",
    "    Then, these terms are used together to compute an overall expected value E[acc].\n",
    "\n",
    "    Args:\n",
    "        accs (torch.Tensor): Accuracies per threshold\n",
    "        counts (torch.Tensor): Counts of samples per threshold\n",
    "        target_accuracy (float): Target accuracy.\n",
    "        order (int): Number of terms in the Hoeffding series for expected value approximation.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Hoeffding expected value for\n",
    "    \"\"\"\n",
    "\n",
    "    valid_mask = accs >= target_accuracy\n",
    "    accs = accs[valid_mask]\n",
    "    counts = counts[valid_mask]\n",
    "\n",
    "    delta = torch.abs(accs - target_accuracy) / 2\n",
    "\n",
    "    probs_list = []\n",
    "    for i in range(1, order + 1):\n",
    "        hoeff_prob = torch.exp(-2 * counts * (i * delta) ** 2)\n",
    "        hoeff_prob = hoeff_prob.clamp(min=0.0, max=1.0)\n",
    "        probs_list.append(hoeff_prob)\n",
    "\n",
    "    expected_value = (1 - probs_list[0]) * (accs - delta)\n",
    "\n",
    "    for i in range(1, order):\n",
    "        exp_term = (probs_list[i - 1] - probs_list[i]) * (accs - (i + 1) * delta)\n",
    "        exp_term = exp_term.clamp(min=0.0, max=1.0)\n",
    "        expected_value += exp_term\n",
    "\n",
    "    result = torch.full_like(valid_mask, fill_value=torch.nan, dtype=expected_value.dtype)\n",
    "    result[valid_mask] = expected_value\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_accuracy = 0.985  # target accuracy\n",
    "series_order = 200  # number of terms in the series\n",
    "eps = 0.001  # max allowed deviation from target accuracy\n",
    "\n",
    "pos_expected = compute_hoeff_expected(\n",
    "    accs=pos_accs,\n",
    "    counts=pos_counts,\n",
    "    target_accuracy=target_accuracy,\n",
    "    order=series_order,\n",
    ")\n",
    "\n",
    "neg_expected = compute_hoeff_expected(\n",
    "    accs=neg_accs,\n",
    "    counts=neg_counts,\n",
    "    target_accuracy=target_accuracy,\n",
    "    order=series_order,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot positive and negative accuracy vs threshold\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(thresholds.cpu(), pos_accs.cpu(), label=\"$\\hat y \\geq thresh$ Accuracy\")\n",
    "plt.plot(thresholds.cpu(), neg_accs.cpu(), label=\"$\\hat y \\leq thresh$ Accuracy\")\n",
    "plt.xlabel(\"$\\hat y $ Threshold\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy vs Threshold\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# plot positive and negative item count vs threshold\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(thresholds.cpu(), pos_counts.cpu(), label=\"$\\hat y \\geq thresh$ Count\")\n",
    "plt.plot(thresholds.cpu(), neg_counts.cpu(), label=\"$\\hat y \\leq thresh$ Count\")\n",
    "plt.xlabel(\"$\\hat y $ Threshold\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Count vs Threshold\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# plot Hoeffding bound vs threshold\n",
    "# plot vertical line at thresholds for which achieves max pos_hoeff at max neg_hoeff values\n",
    "if (pos_expected >= target_accuracy - eps).sum() == 0:\n",
    "    print(\"No bound for pos\")\n",
    "    print(f\"Max pos_expected value: {pos_expected.max()}\")\n",
    "    print(f\"Target accuracy: {target_accuracy}\")\n",
    "    raise ValueError(\"No bound for pos\")\n",
    "\n",
    "if (neg_expected >= target_accuracy - eps).sum() == 0:\n",
    "    print(\"No bound for neg\")\n",
    "    print(f\"Max neg_expected value: {neg_expected.max()}\")\n",
    "    print(f\"Target accuracy: {target_accuracy}\")\n",
    "    raise ValueError(\"No bound for neg\")\n",
    "\n",
    "idx = torch.arange(len(thresholds))\n",
    "pos_idx = torch.min(idx[pos_expected >= target_accuracy - eps])\n",
    "neg_idx = torch.max(idx[neg_expected >= target_accuracy - eps])\n",
    "\n",
    "# find the thresholds for these indices\n",
    "pos_thresh = thresholds[pos_idx]\n",
    "neg_thresh = thresholds[neg_idx]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(thresholds.cpu(), pos_expected.cpu(), label=\"$Pr(\\hat y = y)$ for $\\hat y \\geq thresh$\")\n",
    "plt.plot(thresholds.cpu(), neg_expected.cpu(), label=\"$Pr(\\hat y = y)$ for $\\hat y \\leq thresh$\")\n",
    "plt.axvline(x=pos_thresh.cpu(), color=\"r\", linestyle=\"--\", label=f\"If $\\hat y \\geq thresh$: $Pr(\\hat y = y) > {target_accuracy - eps}$\")\n",
    "plt.axvline(x=neg_thresh.cpu(), color=\"g\", linestyle=\"--\", label=f\"If $\\hat y \\leq thresh$: $Pr(\\hat y = y) > {target_accuracy - eps}$\")\n",
    "\n",
    "plt.xlabel(\"$\\hat y $ Threshold\")\n",
    "plt.ylabel(\"$Pr(\\hat y = y)$\")\n",
    "plt.title(f\"Approximation of $Pr(\\hat y = y) \\geq {target_accuracy}$ per Threshold, with Order {series_order}\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute proportion of covered samples. there are samples for which:\n",
    "# - pred >= pos_thresh\n",
    "# - pred <= neg_thresh\n",
    "\n",
    "covered_mask = (preds >= pos_thresh) | (preds <= neg_thresh)\n",
    "covered_mask = covered_mask.float()\n",
    "covered_mask = covered_mask.sum() / len(preds)\n",
    "\n",
    "print(f\"Covered samples proportion: {covered_mask.item() * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
