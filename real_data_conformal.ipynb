{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader  # Import DataLoader\n",
    "from src.failure_model import ToxicClassifier\n",
    "from src.datasets import PromptOnlyDataset, PropDataset\n",
    "import torch._dynamo\n",
    "\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "try:\n",
    "    torch.multiprocessing.set_start_method(\"spawn\")\n",
    "except RuntimeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model and data\n",
    "\n",
    "ds_prompt_only = PromptOnlyDataset(\"data/rtp_500/split_1_0.5_0.1_0.2_0.2/cal.pkl\")\n",
    "ds_prompt_probs = PropDataset(\"data/rtp_500/split_1_0.5_0.1_0.2_0.2/cal.pkl\", score_name=\"toxicity\", threshold=0.5)\n",
    "\n",
    "ds_prompt_only.data = ds_prompt_only.data[:1500]\n",
    "ds_prompt_probs.data = ds_prompt_probs.data[:1500]\n",
    "\n",
    "dl_prompt_probs = DataLoader(ds_prompt_probs, batch_size=1500, shuffle=False)\n",
    "\n",
    "model = ToxicClassifier.load_from_checkpoint(\"saved/Jigsaw_BERT/lightning_logs/version_1/checkpoints/epoch=4-step=970.ckpt\")\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global config\n",
    "\n",
    "target_taus = torch.tensor([0.1])\n",
    "\n",
    "tau_space = np.logspace(-15, -0.33, 500)\n",
    "taus = torch.tensor(tau_space)\n",
    "\n",
    "target_tau_idx = torch.argmin(torch.abs(taus - target_taus))\n",
    "\n",
    "model.set_taus(taus)\n",
    "model.set_min_p_for_q_tau(1e-20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import utils\n",
    "\n",
    "# Create the parameter dictionary for the rating backend.\n",
    "rater_params = {\n",
    "    \"model_type\": \"original\",\n",
    "    \"amp\": True,\n",
    "}\n",
    "\n",
    "# Get the Hugging Face key.\n",
    "hf_key = utils.api_key_from_file(\"HF_KEY.txt\")\n",
    "\n",
    "# Create the parameter dictionary for the generation backend.\n",
    "generator_params = {\n",
    "    \"model_name\": \"meta-llama/Llama-3.2-1B\",\n",
    "    \"hub_token\": hf_key,\n",
    "    \"max_input_tokens\": 40,\n",
    "    \"max_output_tokens\": 30,\n",
    "    \"torch_dtype\": torch.bfloat16,\n",
    "    # If you need to specify the attention implementation, uncomment the line below.\n",
    "    # \"attn_implementation\": \"flash_attention_2\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.conformal import conformalize\n",
    "\n",
    "# tau_hat, max_est, q_hat = conformalize(\n",
    "#     trainer=pl.Trainer(),\n",
    "#     model=model,\n",
    "#     target_taus=target_taus,\n",
    "#     canidate_taus=taus,\n",
    "#     X=ds_prompt_only,\n",
    "#     generator_params=generator_params,\n",
    "#     rater_params=rater_params,\n",
    "#     budget_per_sample=1000,\n",
    "#     share_budget=True,\n",
    "#     min_sample_size=1,\n",
    "#     text_prep_func=\"sentence_completion\",\n",
    "#     multi_gpu=True,\n",
    "#     # plot=True,\n",
    "#     # return_extra=True,\n",
    "# )\n",
    "\n",
    "result_tuple = conformalize(\n",
    "    trainer=pl.Trainer(),\n",
    "    model=model,\n",
    "    target_taus=target_taus,\n",
    "    canidate_taus=taus,\n",
    "    X=ds_prompt_only,\n",
    "    generator_params=generator_params,\n",
    "    rater_params=rater_params,\n",
    "    budget_per_sample=5,\n",
    "    share_budget=True,\n",
    "    min_sample_size=0.1,\n",
    "    text_prep_func=\"sentence_completion\",\n",
    "    multi_gpu=True,\n",
    "    plot=True,\n",
    "    return_extra=True,\n",
    "    batch_size=1500,\n",
    ")\n",
    "\n",
    "(\n",
    "    tau_hat, # chosen tau for the target miscoverage\n",
    "    max_est, # maximum quantile prediction\n",
    "    q_hats, # quantile predictions for the chosen tau\n",
    "    T_tilde, # sampled survival time for all samples\n",
    "    C, # censoring time indicator\n",
    "    quantile_est, # predicted quantile estimates for all taus\n",
    "    prior_quantile_est, # each output is sampled at most prior_quantile_est times \n",
    "    C_probs, # sampling probability of each sample\n",
    "    weights, # weights used for the weighted miscoverage\n",
    "    miscoverage, # actual miscoverage rate for the GT dist\n",
    ") = result_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_hat_idx = np.argmin(torch.abs(taus - tau_hat)).item()\n",
    "\n",
    "print(\"Tau Hat:\", tau_hat.item())\n",
    "print(\"Tau Hat Miscoverage:\", miscoverage[tau_hat_idx].item())\n",
    "print(\"Max Est:\", max_est)\n",
    "\n",
    "quant_pred = quantile_est[:, target_tau_idx]\n",
    "\n",
    "# Print the average prediction for tau=tau_hat\n",
    "print(\"Avg prediction for tau_hat:\", np.mean(q_hats.clip(max=max_est)))\n",
    "print(\"Avg prediction for tau=0.1:\", np.mean(quant_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
