{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pathlib import Path\n",
                "\n",
                "while Path.cwd().name != 'black-box-api-challenges':\n",
                "    %cd .."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "%reload_ext autoreload\n",
                "%autoreload 2\n",
                "%matplotlib inline\n",
                "\n",
                "import scipy\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt\n",
                "from scipy.stats import linregress\n",
                "\n",
                "from tqdm.auto import tqdm\n",
                "from IPython.display import display\n",
                "from utils.constants import PERSPECTIVE_API_ATTRIBUTES_LOWER\n",
                "\n",
                "sns.set(context=\"paper\", style=\"white\", font_scale=2.0, palette=\"RdBu\")\n",
                "Path(\"images\").mkdir(exist_ok=True, parents=True)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Scores - Original vs. Rescored"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Full sequences"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "full_rescored = pd.read_json(\"data/real-toxicity-prompts/rtp_joint_sequences_rescored.jsonl\", lines=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "bins = list(np.round(np.arange(0, 1.1, 0.25), 2))\n",
                "\n",
                "full_rescored[\"bin\"] = pd.cut(full_rescored[\"toxicity\"], bins=bins)\n",
                "\n",
                "normalize = True\n",
                "full_rescored[\"bin\"].value_counts(normalize=normalize).to_frame().round(2)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Prompts"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "prompt_rescored = pd.read_json(\"data/real-toxicity-prompts/prompts_feb2023.jsonl\", lines=True)\n",
                "original = pd.read_json(\"data/realtoxicityprompts-data/prompts.jsonl\", lines=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "def display_stats(rescored, original, column):\n",
                "    stats = pd.DataFrame({\n",
                "        (\"rescored\", \"toxic\"): (pd.json_normalize(rescored[column])['toxicity'] > 0.5).sum(),\n",
                "        (\"rescored\", \"non-toxic\"): (pd.json_normalize(rescored[column])['toxicity'] <= 0.5).sum(),\n",
                "        (\"original\", \"toxic\"): (pd.json_normalize(original[column])['toxicity'] > 0.5).sum(),\n",
                "        (\"original\", \"non-toxic\"): (pd.json_normalize(original[column])['toxicity'] <= 0.5).sum(),\n",
                "    }, index=[f\"# {column}\"])\n",
                "    display(stats)\n",
                "    stats = pd.DataFrame({\n",
                "        (\"rescored\"): [\n",
                "            pd.json_normalize(rescored[column])['toxicity'].mean(), \n",
                "            pd.json_normalize(rescored[column])['toxicity'].std()],\n",
                "        (\"original\"): [\n",
                "            pd.json_normalize(original[column])['toxicity'].mean(), \n",
                "            pd.json_normalize(original[column])['toxicity'].std()],\n",
                "    }, index=[\"Avg. Toxicity\", \"std\"]).round(2)\n",
                "    display(stats)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "display_stats(prompt_rescored, original, column=\"prompt\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "bins = list(np.round(np.arange(0, 1.1, 0.1), 2))\n",
                "\n",
                "original[\"bin\"] = pd.cut(original.prompt.apply(lambda x: x[\"toxicity\"]), bins=bins)\n",
                "prompt_rescored[\"bin\"] = pd.cut(prompt_rescored.prompt.apply(lambda x: x[\"toxicity\"]), bins=bins)\n",
                "\n",
                "normalize = False\n",
                "df_bins = pd.concat([original[\"bin\"].value_counts(normalize=normalize), prompt_rescored[\"bin\"].value_counts(normalize=normalize)], axis=1).sort_index()\n",
                "df_bins.columns = [\"original\", \"rescored\"]\n",
                "df_bins"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "def process_distributions(original, rescored, attributes, melt=True):\n",
                "    temp = pd.DataFrame()\n",
                "    for attr in attributes:\n",
                "        temp = pd.concat([temp, pd.DataFrame({\n",
                "            \"published\": original[\"prompt\"].apply(lambda x: x[attr]), \n",
                "            \"rescored\": rescored[\"prompt\"].apply(lambda x: x[attr]),\n",
                "            \"attribute\": attr\n",
                "        })])\n",
                "    if melt:\n",
                "        temp = temp.melt(value_vars=[\"published\", \"rescored\"], var_name=\"score\", id_vars=[\"attribute\"])\n",
                "    return temp"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "distributions = process_distributions(original, prompt_rescored, attributes=PERSPECTIVE_API_ATTRIBUTES_LOWER)\n",
                "distributions[\"attribute\"] = distributions[\"attribute\"].str.replace(\"_\", \" \")\n",
                "\n",
                "plt.figure(figsize=(15, 6))\n",
                "sns.violinplot(data=distributions, x=\"attribute\", y=\"value\", hue=\"score\", split=True, inner=None)\n",
                "plt.xticks(rotation=45)\n",
                "plt.xlabel(\"Perspective API attributes density plots\")\n",
                "plt.ylabel(\"Perspective API scores\")\n",
                "plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
                "plt.tight_layout()\n",
                "plt.savefig('images/rtp_prompts_attr_distributions.svg', format=\"svg\")\n",
                "plt.savefig('images/rtp_prompts_attr_distributions.pdf')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "distributions = process_distributions(original, prompt_rescored, attributes=PERSPECTIVE_API_ATTRIBUTES_LOWER, melt=False).dropna()\n",
                "distributions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "distributions.groupby(\"attribute\").apply(\n",
                "    lambda x: scipy.stats.wasserstein_distance(\n",
                "        x['published'].values, \n",
                "        x['rescored'].values, \n",
                "    )\n",
                ").round(3).sort_values()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def kl_divergence(p, q):\n",
                "    return np.sum(np.where(p != 0, p * np.log(p / q), 0))\n",
                "\n",
                "distributions.groupby(\"attribute\").apply(\n",
                "    lambda x: kl_divergence(\n",
                "        x['published'].values, \n",
                "        x['rescored'].values, \n",
                "    )\n",
                ").round(3).sort_values()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Qualitative Assessment of Prompts"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "from wordcloud import WordCloud\n",
                "from wordcloud import STOPWORDS\n",
                "import string"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_wordcloud(df):\n",
                "    text = \" \".join(i for i in df.text)\n",
                "    stopwords = set(list(STOPWORDS) + list(string.ascii_lowercase) + list(string.ascii_uppercase))\n",
                "    wordcloud = WordCloud(\n",
                "        stopwords=stopwords, \n",
                "        background_color=\"white\",\n",
                "        collocations=False,\n",
                "        normalize_plurals=True,\n",
                "        width=1000,\n",
                "        height=1500, \n",
                "        random_state=42,\n",
                "        colormap=\"copper\"\n",
                "    ).generate(text)\n",
                "    plt.figure(figsize=(5, 7))\n",
                "    plt.imshow(wordcloud, interpolation='bilinear')\n",
                "    plt.axis(\"off\")\n",
                "    return plt\n",
                "\n",
                "def display_samples(df):\n",
                "    display(\"Top-10 samples with bigger abs difference.\")\n",
                "    display(df.sort_values(\n",
                "        by=\"abs_diff\", \n",
                "        ascending=False\n",
                "        ).iloc[:10][[\"text\", \"original\", \"rescored\", \"abs_diff\"]].round(2).set_index(\"text\", drop=True)\n",
                "    )\n",
                "\n",
                "    display(\"Top-10 samples with smaller abs difference.\")\n",
                "    display(df.sort_values(\n",
                "        by=\"abs_diff\", \n",
                "        ascending=True\n",
                "        ).iloc[:10][[\"text\", \"original\", \"rescored\", \"abs_diff\"]].round(2).set_index(\"text\", drop=True)\n",
                "    )\n",
                "    g = sns.regplot(data=df, x=\"original\", y=\"rescored\")\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "threshold = 0.5\n",
                "prompts = pd.json_normalize(original[\"prompt\"])[[\"text\", \"toxicity\"]].rename(columns={\"toxicity\": \"original\"})\n",
                "rescored_prompts = pd.json_normalize(prompt_rescored[\"prompt\"])[[\"text\", \"toxicity\"]]\n",
                "\n",
                "assert prompts[\"text\"].equals(rescored_prompts[\"text\"])\n",
                "prompts[\"rescored\"] = rescored_prompts[\"toxicity\"]\n",
                "\n",
                "pd.crosstab(prompts[\"original\"] > threshold, prompts[\"rescored\"] > threshold)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "new_toxic = prompts.query(\"original <= @threshold and rescored > @threshold\")\n",
                "new_nontoxic = prompts.query(\"rescored <= @threshold and original > @threshold\")\n",
                "\n",
                "new_toxic[\"abs_diff\"] = np.abs(new_toxic.rescored.values - new_toxic.original.values)\n",
                "new_nontoxic[\"abs_diff\"] = np.abs(new_nontoxic.rescored.values - new_nontoxic.original.values)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [],
            "source": [
                "path = Path(\"data/qualitative/prompts_new_toxic.csv\")\n",
                "path.parent.mkdir(parents=True, exist_ok=True)\n",
                "new_toxic[[\"text\"]].set_index(\"text\").to_csv(path)\n",
                "\n",
                "path = Path(\"data/qualitative/prompts_new_nontoxic.csv\")\n",
                "new_nontoxic[[\"text\"]].set_index(\"text\").to_csv(path)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### New Toxic Prompts"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plot_wordcloud(new_toxic.copy())\n",
                "plt.tight_layout()\n",
                "plt.savefig(\"images/wordcloud_prompts_new_toxic.pdf\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pd.set_option('display.max_colwidth', None)\n",
                "\n",
                "display_samples(new_toxic.copy())"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### New Non-Toxic prompts"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plot_wordcloud(new_nontoxic.copy())\n",
                "plt.tight_layout()\n",
                "plt.savefig(\"images/wordcloud_prompts_new_nontoxic.pdf\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "display_samples(new_nontoxic.copy())"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Continuations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [],
            "source": [
                "cont_rescored = pd.read_json(\"data/real-toxicity-prompts/rtp_continuations_rescored.jsonl\", lines=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "display_stats(cont_rescored, original, column=\"continuation\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Prompted scores - Baselines from RealToxicityPrompts\n",
                "\n",
                "From Table 2. Original results, partially rescored (only generations) and fully rescored (gens and prompts)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [],
            "source": [
                "prompted_models = {\n",
                "    \"GPT1\": {\n",
                "        \"original\": \"data/real-toxicity-prompts/rtp_generations/prompted_gens_gpt1_original_toxicities.csv\",\n",
                "        \"all rescored\": \"data/real-toxicity-prompts/rtp_generations/prompted_gens_gpt1_all_rescored_toxicities.csv\",\n",
                "        \"generations rescored\": \"data/real-toxicity-prompts/rtp_generations/prompted_gens_gpt1_gens_rescored_toxicities.csv\",\n",
                "    },\n",
                "    \"GPT2\": {\n",
                "        \"original\": \"data/real-toxicity-prompts/rtp_generations/prompted_gens_gpt2_original_toxicities.csv\",\n",
                "        \"all rescored\": \"data/real-toxicity-prompts/rtp_generations/prompted_gens_gpt2_all_rescored_toxicities.csv\",\n",
                "        \"generations rescored\": \"data/real-toxicity-prompts/rtp_generations/prompted_gens_gpt2_gens_rescored_toxicities.csv\",\n",
                "    },\n",
                "    \"GPT3\": {\n",
                "        \"original\": \"data/real-toxicity-prompts/rtp_generations/prompted_gens_gpt3_davinci_original_toxicities.csv\",\n",
                "        \"all rescored\": \"data/real-toxicity-prompts/rtp_generations/prompted_gens_gpt3_davinci_all_rescored_toxicities.csv\",\n",
                "        \"generations rescored\": \"data/real-toxicity-prompts/rtp_generations/prompted_gens_gpt3_davinci_gens_rescored_toxicities.csv\",\n",
                "    },\n",
                "    \"CTRL\": {\n",
                "        \"original\": \"data/real-toxicity-prompts/rtp_generations/prompted_gens_ctrl_original_toxicities.csv\",\n",
                "        \"all rescored\": \"data/real-toxicity-prompts/rtp_generations/prompted_gens_ctrl_all_rescored_toxicities.csv\",\n",
                "        \"generations rescored\": \"data/real-toxicity-prompts/rtp_generations/prompted_gens_ctrl_gens_rescored_toxicities.csv\",\n",
                "    },\n",
                "    \"CTRL-W\": {\n",
                "        \"original\": \"data/real-toxicity-prompts/rtp_generations/prompted_gens_ctrl_wiki_original_toxicities.csv\",\n",
                "        \"all rescored\": \"data/real-toxicity-prompts/rtp_generations/prompted_gens_ctrl_wiki_all_rescored_toxicities.csv\",\n",
                "        \"generations rescored\": \"data/real-toxicity-prompts/rtp_generations/prompted_gens_ctrl_wiki_gens_rescored_toxicities.csv\",\n",
                "    },\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_toxicity(paths_dict):\n",
                "    tox_metrics = pd.DataFrame()\n",
                "    for model, metrics in paths_dict.items():\n",
                "        for mode, path in metrics.items():\n",
                "            if not Path(path).exists():\n",
                "                continue\n",
                "            metrics = pd.read_csv(path).T\n",
                "            metrics.columns = metrics.iloc[0].values.tolist()\n",
                "            metrics = metrics[1:]\n",
                "            metrics[\"model\"] = model\n",
                "            metrics[\"mode\"] = mode\n",
                "            tox_metrics = pd.concat([tox_metrics, metrics])\n",
                "    tox_metrics.index.name = \"prompt_toxicity\"\n",
                "    return tox_metrics.reset_index()\n",
                "\n",
                "def show_toxicity_results(df, mode, row_order=None, round=2):\n",
                "    df = df.replace(to_replace={\"toxic\": \"Toxic\", \"nontoxic\": \"Non-Toxic\"})\n",
                "    df = df.rename(columns={\n",
                "        \"avg_max\": \"Exp. Max. Toxicity\", \n",
                "        \"toxicity_probability\": \"Toxicity Probability\", \n",
                "        \"toxic_fraction\": \"Toxic Fraction\",\n",
                "        \"model\": \"Model\"\n",
                "    })\n",
                "\n",
                "    metrics = [\"Exp. Max. Toxicity\", \"std_max\", \"Toxicity Probability\"]\n",
                "    if 'Toxic Fraction' in df.columns:\n",
                "        metrics += [\"Toxic Fraction\"]\n",
                "    df = pd.pivot_table(\n",
                "        df.query(\"mode == @mode and prompt_toxicity != 'full'\"), \n",
                "        index=[\"Model\"], \n",
                "        values=metrics, columns=[\"prompt_toxicity\"]\n",
                "    ).round(round)\n",
                "\n",
                "    # Put toxic first\n",
                "    df = df.reindex([\"Toxic\", \"Non-Toxic\"], axis=1, level=1)\n",
                "\n",
                "    # Reorder rows to match paper results\n",
                "    if row_order is not None:\n",
                "        df = df.reindex(row_order)\n",
                "\n",
                "    return df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [],
            "source": [
                "toxicity_metrics = load_toxicity(prompted_models)\n",
                "row_order = [\"GPT1\", \"GPT2\", \"GPT3\", \"CTRL\", \"CTRL-W\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "original_res = show_toxicity_results(toxicity_metrics, mode=\"original\", row_order=row_order)\n",
                "original_res"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "gens_res = show_toxicity_results(toxicity_metrics, mode=\"generations rescored\", row_order=row_order)\n",
                "gens_res"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "all_res = show_toxicity_results(toxicity_metrics, mode=\"all rescored\", row_order=row_order)\n",
                "all_res"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [],
            "source": [
                "orig_melt = original_res.reset_index().melt(id_vars=[\"Model\"])\n",
                "orig_melt[\"mode\"] = \"original\"\n",
                "\n",
                "gens_melt = gens_res.reset_index().melt(id_vars=[\"Model\"])\n",
                "gens_melt[\"mode\"] = \"generations rescored\"\n",
                "\n",
                "all_melt = all_res.reset_index().melt(id_vars=[\"Model\"])\n",
                "all_melt[\"mode\"] = \"all rescored\"\n",
                "\n",
                "rtp_baselines = pd.concat([orig_melt, gens_melt, all_melt]).reset_index(drop=True)\n",
                "rtp_baselines = rtp_baselines.rename(columns={None: \"Metric\", \"prompt_toxicity\": \"Prompt\"})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "temp = rtp_baselines.query(\"Metric != 'std_max'\")\n",
                "g = sns.catplot(\n",
                "    data=temp,\n",
                "    x=\"mode\", y='value', hue=\"Model\",\n",
                "    col=\"Prompt\", \n",
                "    row=\"Metric\",\n",
                "    kind=\"point\", \n",
                "    sharex=True, \n",
                "    sharey=True,\n",
                "    height=5,\n",
                "    scale=2.0\n",
                ")\n",
                "g.set_titles(template=\"Prompt = {col_name}\")\n",
                "g.set_xticklabels([\"published\", \"generations\\nrescored\", \"all\\nrescored\"], fontsize=16)\n",
                "g.set_xlabels(\"\", \"\")\n",
                "g.set_xlabels(\"\", \"\")\n",
                "\n",
                "metrics = [\"Exp. Max. Toxicity\", \"Toxicity Probability\"]\n",
                "for i in range(len(metrics)):\n",
                "    g.axes[i, 0].set_ylabel(metrics[i])\n",
                "    if i > 0:\n",
                "        g.axes[i, 0].set_title(\"\")\n",
                "        g.axes[i, 1].set_title(\"\")\n",
                "\n",
                "g.savefig(f\"images/rtp_baselines.svg\", format=\"svg\")\n",
                "g.savefig(f\"images/rtp_baselines.pdf\")\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for i, metric in enumerate([\"Exp. Max. Toxicity\", \"Toxicity Probability\"]):\n",
                "    temp = rtp_baselines.query(\"Metric == @metric\")\n",
                "    g = sns.catplot(\n",
                "        data=temp,\n",
                "        x=\"mode\", \n",
                "        y='value', \n",
                "        hue=\"Model\",\n",
                "        col=\"Prompt\", \n",
                "        kind=\"point\", \n",
                "        sharex=True, \n",
                "        sharey=True,\n",
                "        height=4,\n",
                "        scale=1.5\n",
                "    )\n",
                "    g.figure.subplots_adjust(top=0.82)\n",
                "    g.set_titles(template=\"Prompt = {col_name}\", size=14)\n",
                "    g.set_xticklabels([\"published\", \"generations\\nrescored\", \"prompts and\\ngenerations\\nrescored\"], fontsize=11)\n",
                "    g.set(ylim=(0, 1.0))\n",
                "    # g.set_yticklabels([f'{val:.2f}' for val in np.arange(0, 1.1, 0.25)], fontsize=11)\n",
                "    g.set_xlabels(\"\", \"\")\n",
                "    g.set_ylabels(\"\", \"\")\n",
                "\n",
                "    sns.move_legend(g, \"upper center\", ncols=5, fontsize=11, title=\"\")\n",
                "    if i == 0:\n",
                "        g.axes[0, i].set(ylim=(0.3, 0.9))\n",
                "        # g.legend.remove()\n",
                "\n",
                "    g.savefig(f\"images/rtp_baselines_{metric}.pdf\")\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Prompted Generations - Other papers"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### DExperts\n",
                "\n",
                "Only models showcased in UDDIA's paper table 3."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [],
            "source": [
                "dexperts_models = {\n",
                "    \"GPT2 (large)\": {\n",
                "        \"original\": \"data/dexperts/generations/toxicity/toxicity/prompted_gens_gpt2_original_toxicity.csv\",\n",
                "        \"generations rescored\": \"data/dexperts/generations/toxicity/toxicity/prompted_gens_gpt2_gens_rescored_toxicity.csv\",\n",
                "        \"all rescored\": \"data/dexperts/generations/toxicity/toxicity/prompted_gens_gpt2_all_rescored_toxicity.csv\"\n",
                "    },\n",
                "    \"DAPT\": {\n",
                "        \"original\": \"data/dexperts/generations/toxicity/toxicity/prompted_gens_dapt_original_toxicity.csv\",\n",
                "        \"generations rescored\": \"data/dexperts/generations/toxicity/toxicity/prompted_gens_dapt_gens_rescored_toxicity.csv\",\n",
                "        \"all rescored\": \"data/dexperts/generations/toxicity/toxicity/prompted_gens_dapt_all_rescored_toxicity.csv\"\n",
                "    },\n",
                "    \"GeDi\": {\n",
                "        \"original\": \"data/dexperts/generations/toxicity/toxicity/prompted_gens_gedi_original_toxicity.csv\",\n",
                "        \"generations rescored\": \"data/dexperts/generations/toxicity/toxicity/prompted_gens_gedi_gens_rescored_toxicity.csv\",\n",
                "        \"all rescored\": \"data/dexperts/generations/toxicity/toxicity/prompted_gens_gedi_all_rescored_toxicity.csv\"\n",
                "    },\n",
                "    \"DExperts (large)\": {\n",
                "        \"original\": \"data/dexperts/generations/toxicity/toxicity/prompted_gens_dexperts_large_original_toxicity.csv\",\n",
                "        \"generations rescored\": \"data/dexperts/generations/toxicity/toxicity/prompted_gens_dexperts_large_gens_rescored_toxicity.csv\",\n",
                "        \"all rescored\": \"data/dexperts/generations/toxicity/toxicity/prompted_gens_dexperts_large_all_rescored_toxicity.csv\"\n",
                "    },\n",
                "    \"PPLM (10%)\": {\n",
                "        \"original\": \"data/dexperts/generations/toxicity/toxicity/prompted_gens_pplm_original_toxicity.csv\",\n",
                "        \"generations rescored\": \"data/dexperts/generations/toxicity/toxicity/prompted_gens_pplm_gens_rescored_toxicity.csv\",\n",
                "        \"all rescored\": \"data/dexperts/generations/toxicity/toxicity/prompted_gens_pplm_all_rescored_toxicity.csv\"\n",
                "    },\n",
                "    \"UDDIA (TH=40)\": {\n",
                "        \"original\": \"data/uddia/continuations/TH40/published_toxicity.csv\",\n",
                "        \"generations rescored\": \"data/uddia/continuations/TH40/toxicity_dict.csv\",\n",
                "    },\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {},
            "outputs": [],
            "source": [
                "dexperts_tox = load_toxicity(dexperts_models)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dexperts_tox"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "original_res = show_toxicity_results(dexperts_tox, mode=\"original\", round=3)\n",
                "original_res"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "gens_res = show_toxicity_results(dexperts_tox, mode=\"generations rescored\", round=3)\n",
                "gens_res"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "all_res = show_toxicity_results(dexperts_tox, mode=\"all rescored\", round=3)\n",
                "all_res"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "metadata": {},
            "outputs": [],
            "source": [
                "orig_melt = original_res.reset_index().melt(id_vars=[\"Model\"])\n",
                "orig_melt[\"mode\"] = \"original\"\n",
                "\n",
                "gens_melt = gens_res.reset_index().melt(id_vars=[\"Model\"])\n",
                "gens_melt[\"mode\"] = \"generations rescored\"\n",
                "\n",
                "# Removed 'all' from plot since no major changes in results\n",
                "dexperts_baselines = pd.concat([orig_melt, gens_melt]).reset_index(drop=True)\n",
                "dexperts_baselines = dexperts_baselines.rename(columns={None: \"Metric\", \"prompt_toxicity\": \"Prompt\"})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "temp = dexperts_baselines.query(\"Metric != 'std_max' and Prompt == 'Non-Toxic'\")\n",
                "\n",
                "g = sns.catplot(\n",
                "    data=temp,\n",
                "    x=\"mode\", \n",
                "    y=\"value\", \n",
                "    hue=\"Model\",\n",
                "    col=\"Metric\",\n",
                "    kind=\"point\", \n",
                "    sharex=True, \n",
                "    sharey=False,\n",
                "    height=5,\n",
                "    scale=2.2\n",
                ")\n",
                "g.set_titles(template=\"{col_name}\")\n",
                "g.set_xticklabels([\"published\", \"generations\\nrescored\"])\n",
                "g.set_xlabels(\"\", \"\")\n",
                "for axis in g.axes.flat:\n",
                "    axis.tick_params(labelleft=True)\n",
                "sns.move_legend(g, \"upper center\", ncols=6, fontsize=12, title=\"\", bbox_to_anchor=(0.5, 1.1))\n",
                "plt.tight_layout()\n",
                "g.savefig(f\"images/uddia_results.svg\", format=\"svg\")\n",
                "g.savefig(f\"images/uddia_results.pdf\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "temp = dexperts_baselines.query(\"Metric != 'std_max' and Prompt == 'Non-Toxic'\")\n",
                "temp[\"normalized value\"] = temp.groupby([\"mode\", \"Metric\"])[\"value\"].transform(\n",
                "    lambda x: (x - x.min()) / (x.max() - x.min()))\n",
                "temp[\"slope\"] = temp.groupby([\"Model\", \"Metric\"])[\"normalized value\"].transform(lambda x: linregress([0, 1], [x.iloc[0], x.iloc[1]]).slope).round(2)\n",
                "\n",
                "g = sns.catplot(\n",
                "    data=temp,\n",
                "    x=\"mode\", \n",
                "    y=\"normalized value\", \n",
                "    hue=\"Model\",\n",
                "    col=\"Metric\",\n",
                "    kind=\"point\", \n",
                "    sharex=True, \n",
                "    sharey=False,\n",
                "    height=5,\n",
                "    scale=2.2\n",
                ")\n",
                "g.set_titles(template=\"{col_name}\")\n",
                "g.set_xticklabels([\"published\", \"generations\\nrescored\"])\n",
                "g.set_xlabels(\"\", \"\")\n",
                "for col, metric in enumerate(g.col_names):\n",
                "    ax = g.axes[0, col]\n",
                "    for c in ax.collections:\n",
                "        offsets = c.get_offsets()\n",
                "        # Annotate just next to second dot\n",
                "        slope = linregress(offsets.data.T).slope\n",
                "\n",
                "        x_bump = 0.1\n",
                "        y_bump = -0.05  if slope > 0 else 0\n",
                "        ax.annotate(f\"{slope:.2f}\", offsets[1, :] + [x_bump, y_bump], fontsize=12) \n",
                "\n",
                "sns.move_legend(g, \"upper center\", ncols=6, fontsize=12, title=\"\", bbox_to_anchor=(0.5, 1.1))\n",
                "plt.tight_layout()\n",
                "\n",
                "g.savefig(f\"images/uddia_slopes.svg\", format=\"svg\")\n",
                "g.savefig(f\"images/uddia_slopes.pdf\")\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### How is the distribution of 10k non-toxic sample from DExperts now?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "metadata": {},
            "outputs": [],
            "source": [
                "dexperts_prompts_original = pd.read_json(\"data/dexperts/prompts/nontoxic_prompts-10k.jsonl\", lines=True)\n",
                "dexperts_prompts_rescored = pd.read_json(\"data/dexperts/prompts/nontoxic_prompts-10k_rescored.jsonl\", lines=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "metadata": {},
            "outputs": [],
            "source": [
                "toxicity = pd.DataFrame({\n",
                "    \"toxic_original\": dexperts_prompts_original.prompt.apply(lambda x: x.get(\"toxicity\")) > 0.5,\n",
                "    \"toxic_rescored\": dexperts_prompts_rescored.prompt.apply(lambda x: x.get(\"toxicity\")) > 0.5\n",
                "})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pd.crosstab(toxicity[\"toxic_original\"], toxicity[\"toxic_rescored\"])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "llm-survival",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}