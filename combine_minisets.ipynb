{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4191168f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "from src.survival_runner import SurvivalResult\n",
    "from src.rating.base import RatingResult\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0ee094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class DatasetType(Enum):\n",
    "    FULL = 0\n",
    "    \"\"\"\n",
    "    Dataset type: `list[SurvivalResult]`   \n",
    "    Full dataset containing:\n",
    "    - input prompt\n",
    "    - all model response\n",
    "    - all model response ratings\n",
    "    \"\"\"\n",
    "     \n",
    "    FULL_LIGHT = 1\n",
    "    \"\"\"\n",
    "    Dataset type: `list[SurvivalResult]`   \n",
    "    Light weight version of the full dataset containing:\n",
    "    - input prompt\n",
    "    - all model response ratings\n",
    "    \"\"\"\n",
    "    \n",
    "    PROMPT_ONLY = 2\n",
    "    \"\"\"\n",
    "    Dataset type: `list[SurvivalResult]`   \n",
    "    Dataset containing:\n",
    "    - input prompt\n",
    "    \"\"\"\n",
    "    \n",
    "    SURV_ONLY = 3\n",
    "    \"\"\"\n",
    "    Dataset type: `np.ndarray`   \n",
    "    A numpy file dataset containing:\n",
    "    - survival time for each prompt\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4938df36",
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments = [\n",
    "    \"mini_datasets_mine/mini_set_0.pkl\",\n",
    "    \"mini_datasets_mine/mini_set_1.pkl\",\n",
    "    \"mini_datasets_mine/mini_set_2.pkl\",\n",
    "    \"mini_datasets_mine/mini_set_3.pkl\",\n",
    "    \"mini_datasets_mine/mini_set_4.pkl\",\n",
    "    \"mini_datasets_mine/mini_set_5.pkl\",\n",
    "    \"mini_datasets_mine/mini_set_6.pkl\",\n",
    "    \"mini_datasets_mine/mini_set_7.pkl\",\n",
    "    \"mini_datasets_mine/mini_set_8.pkl\",\n",
    "    \"mini_datasets_mine/mini_set_9.pkl\",\n",
    "    \"mini_datasets_mine/mini_set_10.pkl\",\n",
    "    \"mini_datasets_mine/mini_set_11.pkl\",\n",
    "    \"mini_datasets_mine/mini_set_12.pkl\",\n",
    "    \"mini_datasets_mine/mini_set_13.pkl\",\n",
    "    \"mini_datasets_mine/mini_set_14.pkl\",\n",
    "    \"mini_datasets_mine/mini_set_15.pkl\",\n",
    "    \"mini_datasets_mine/mini_set_16.pkl\",\n",
    "    \"mini_datasets_mine/mini_set_17.pkl\",\n",
    "    \"mini_datasets_mine/mini_set_18.pkl\",\n",
    "    \"mini_datasets/mini_set_0.pkl\",\n",
    "    \"mini_datasets/mini_set_1.pkl\",\n",
    "    \"mini_datasets/mini_set_2.pkl\",\n",
    "    \"mini_datasets/mini_set_3.pkl\",\n",
    "    \"mini_datasets/mini_set_4.pkl\",\n",
    "]\n",
    "    \n",
    "# paths for each dataset type\n",
    "ds_full_path = \"data/test_full.pkl\"\n",
    "ds_full_light_path = \"data/test_full_light.pkl\"\n",
    "ds_prompt_only_path = \"data/test_prompt_only.pkl\"\n",
    "ds_surv_times_path = \"data/test_surv_times.npy\"\n",
    "\n",
    "\n",
    "# dataset type to create\n",
    "ds_type = DatasetType.FULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c753757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that every all files are different, by comparing file size\n",
    "\n",
    "def check_files(fragments):\n",
    "    file_sizes = {}\n",
    "    for fragment in fragments:\n",
    "        size = os.path.getsize(fragment)\n",
    "        if size in file_sizes:\n",
    "            print(f\"Duplicate file size found: {size} in {file_sizes[size]} and {fragment}\")\n",
    "        else:\n",
    "            file_sizes[size] = fragment\n",
    "    print(\"All files are unique in size.\")\n",
    "    return file_sizes\n",
    "\n",
    "check_files(fragments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5976a1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data: list[list[SurvivalResult]] = []\n",
    "for i, fragment in tqdm(enumerate(fragments), desc=\"Loading fragments\"):\n",
    "    all_data.append(pickle.load(open(fragment, \"rb\")))\n",
    "    print(f\"Loaded {len(all_data[-1])} results from {fragment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78d12a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram of all fragments\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_first_toxicity_index(res: SurvivalResult) -> int:\n",
    "    for i, rating in enumerate(res.ratings):\n",
    "        if rating.scores[\"toxicity\"] > 0.5:\n",
    "            return i\n",
    "    return len(res.ratings)\n",
    "\n",
    "def plot_fragment(fragment: list[SurvivalResult]) -> list[int]:\n",
    "    surv = [get_first_toxicity_index(res) for res in fragment]\n",
    "    plt.hist(surv, histtype=\"step\")\n",
    "\n",
    "for i, fragment in tqdm(enumerate(all_data), desc=\"Plotting fragments\"):\n",
    "    plot_fragment(fragment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f5376f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now joint all fragments\n",
    "\n",
    "joint_data: list[SurvivalResult] = []\n",
    "\n",
    "for i in tqdm(range(len(all_data[0])), desc=\"Joining fragments\"):\n",
    "    \n",
    "    # assert prompt of fr[i] for all fr is the same\n",
    "    for j in range(1, len(all_data)):\n",
    "        assert all_data[0][i].prompt == all_data[j][i].prompt, f\"Prompt mismatch at index {i} in fragment {j}\"\n",
    "        \n",
    "    survival_results = [fr[i] for fr in all_data]\n",
    "\n",
    "    new_id = i\n",
    "    prompt = survival_results[0].prompt\n",
    "\n",
    "    max_attempts = sum(res.max_attempts for res in survival_results)\n",
    "    num_attempts = max_attempts\n",
    "    is_toxic = any(res.is_toxic for res in survival_results)\n",
    "\n",
    "    # join ratings\n",
    "    ratings = []\n",
    "    for res in survival_results:\n",
    "        ratings.extend(res.ratings)\n",
    "\n",
    "    new_surv = SurvivalResult(\n",
    "        id=new_id,\n",
    "        prompt=prompt,\n",
    "        max_attempts=max_attempts,\n",
    "        num_attempts=num_attempts,\n",
    "        is_toxic=is_toxic,\n",
    "        ratings=ratings,\n",
    "    )\n",
    "\n",
    "    joint_data.append(new_surv)\n",
    "    \n",
    "if ds_type == DatasetType.FULL:\n",
    "    pickle.dump(joint_data, open(ds_full_path, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cb38e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make light-version of the dataset\n",
    "\n",
    "joint_data_light = []\n",
    "\n",
    "for surv in tqdm(joint_data):\n",
    "\n",
    "    new_ratings = [RatingResult(text=\"\", scores=rating.scores) for rating in surv.ratings]\n",
    "\n",
    "    light_surv = SurvivalResult(\n",
    "        id=surv.id,\n",
    "        prompt=surv.prompt,\n",
    "        max_attempts=surv.max_attempts,\n",
    "        num_attempts=surv.num_attempts,\n",
    "        is_toxic=surv.is_toxic,\n",
    "        ratings=new_ratings,\n",
    "    )\n",
    "\n",
    "    joint_data_light.append(light_surv)\n",
    "    \n",
    "if ds_type == DatasetType.FULL_LIGHT:\n",
    "    pickle.dump(joint_data_light, open(ds_full_light_path, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df20f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make super-light version of the dataset with only prompts\n",
    "\n",
    "joint_data_prompt_only = []\n",
    "\n",
    "for surv in tqdm(joint_data):\n",
    "\n",
    "    light_surv = SurvivalResult(\n",
    "        id=surv.id,\n",
    "        prompt=surv.prompt,\n",
    "        max_attempts=surv.max_attempts,\n",
    "        num_attempts=surv.num_attempts,\n",
    "        is_toxic=surv.is_toxic,\n",
    "    )\n",
    "\n",
    "    joint_data_prompt_only.append(light_surv)\n",
    "    \n",
    "if ds_type == DatasetType.PROMPT_ONLY:\n",
    "    pickle.dump(joint_data_prompt_only, open(ds_prompt_only_path, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68749bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate\n",
    "\n",
    "from src.datasets import PromptOnlyDataset, SurvivalDataset\n",
    "\n",
    "ds_surv = SurvivalDataset(ds_full_path, score_name=\"toxicity\", threshold=0.5)\n",
    "ds_surv_light = SurvivalDataset(ds_full_light_path, score_name=\"toxicity\", threshold=0.5)\n",
    "ds_prompt = PromptOnlyDataset(ds_full_path)\n",
    "\n",
    "# make sure that the datasets are the same\n",
    "for i in tqdm(range(len(ds_surv)), desc=\"Checking datasets\"):\n",
    "    x_surv, y_surv = ds_surv[i]\n",
    "    x_surv_light, y_surv_light = ds_surv_light[i]\n",
    "    \n",
    "    assert x_surv == x_surv_light, f\"Dataset prompt mismatch at index {i} in survival dataset\"\n",
    "    assert y_surv[0] == y_surv_light[0], f\"Dataset label mismatch at index {i} in survival dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29aa237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "surv_times = [item[1][0] for item in ds_surv]\n",
    "surv_times = np.asanyarray(surv_times)\n",
    "\n",
    "if ds_type == DatasetType.SURV_ONLY:\n",
    "    np.save(ds_surv_times_path, surv_times)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
