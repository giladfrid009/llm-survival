{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/fre.gilad/miniforge3/envs/vllm4/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/fre.gilad/miniforge3/envs/vllm4/lib/python3.11 ...\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/fre.gilad/miniforge3/envs/vllm4/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a74f6275cc14e2fb30ea7622b59f107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the lightning model from checkpoint\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch    \n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader  # Import DataLoader\n",
    "from src.failure_model import ToxicClassifier\n",
    "from src.datasets import PromptOnlyDataset, PropDataset\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "try:\n",
    "    torch.multiprocessing.set_start_method('spawn')\n",
    "except RuntimeError:\n",
    "    pass\n",
    "\n",
    "cal_prompts = PromptOnlyDataset(\"data/rtp_500/split_1_0.5_0.1_0.2_0.2/cal.pkl\")\n",
    "\n",
    "cal_props = PropDataset(\"data/rtp_500/split_1_0.5_0.1_0.2_0.2/cal.pkl\", score_name=\"toxicity\", threshold=0.5)\n",
    "\n",
    "# cal_prompts.data = cal_prompts.data[-100:]\n",
    "# cal_props.data = cal_props.data[-100:]\n",
    "\n",
    "# Examine the model weights in the checkpoint before loading\n",
    "model_path = \"saved/Prop_rtp_500_ModernBERT/lightning_logs/version_0/checkpoints/epoch=4-step=495.ckpt\"\n",
    "model = ToxicClassifier.load_from_checkpoint(model_path)\n",
    "\n",
    "taus = torch.tensor(np.logspace(-3, 0, 200))\n",
    "# Get the index closest to 0.1\n",
    "idx = (taus - 0.1).abs().argmin()\n",
    "model.set_taus(taus)\n",
    "model.set_min_p_for_q_tau(1e-20)\n",
    "# model.set_threshold_p_for_q_tau(1e-4)\n",
    "model.eval()\n",
    "\n",
    "trainer = pl.Trainer()\n",
    "\n",
    "# Create a custom DataLoader for prediction with a batch size of 1500\n",
    "predict_dataloader = DataLoader(cal_prompts, batch_size=1500, shuffle=False)\n",
    "\n",
    "# Use the trainer and predict on cal_prompts using the custom DataLoader\n",
    "pred = trainer.predict(model, dataloaders=predict_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# cal_path = \"data/rtp_500/split_1_0.5_0.1_0.2_0.2/cal.pkl\"\n",
    "\n",
    "# with open(cal_path, \"rb\") as f:\n",
    "#     raw_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Overhead tokens:  100\n",
      "INFO: Empty input tokens:  1\n",
      "INFO: Total sequence tokens:  151\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c68ebb08d20843a68b3f517b455f7dee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/844 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-28 22:43:38 [config.py:585] This model supports multiple tasks: {'generate', 'score', 'reward', 'classify', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 04-28 22:43:38 [config.py:1697] Chunked prefill is enabled with max_num_batched_tokens=151100.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57b30fee6877452792804354a3d3f49c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-28 22:43:39 [utils.py:2181] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "INFO 04-28 22:43:44 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 04-28 22:43:46 [core.py:54] Initializing a V1 LLM engine (v0.8.2) with config: model='meta-llama/Llama-3.2-3B', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=251, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.2-3B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 04-28 22:43:47 [utils.py:2321] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fa7e67d9ad0>\n",
      "INFO 04-28 22:43:47 [parallel_state.py:954] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 04-28 22:43:47 [cuda.py:220] Using Flash Attention backend on V1 engine.\n",
      "INFO 04-28 22:43:47 [gpu_model_runner.py:1174] Starting to load model meta-llama/Llama-3.2-3B...\n",
      "WARNING 04-28 22:43:47 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 04-28 22:43:48 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "INFO 04-28 22:45:09 [weight_utils.py:281] Time spent downloading weights for meta-llama/Llama-3.2-3B: 80.995300 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:32<00:32, 32.63s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:41<00:00, 18.75s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:41<00:00, 20.83s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-28 22:45:51 [loader.py:447] Loading weights took 41.70 seconds\n",
      "INFO 04-28 22:45:52 [gpu_model_runner.py:1186] Model loading took 6.0169 GB and 124.386554 seconds\n",
      "INFO 04-28 22:46:00 [backends.py:415] Using cache directory: /home/fre.gilad/.cache/vllm/torch_compile_cache/d7405ee669/rank_0_0 for vLLM's torch.compile\n",
      "INFO 04-28 22:46:00 [backends.py:425] Dynamo bytecode transform time: 8.55 s\n",
      "INFO 04-28 22:46:04 [backends.py:132] Cache the graph of shape None for later use\n",
      "INFO 04-28 22:46:29 [backends.py:144] Compiling a graph for general shape takes 28.42 s\n",
      "INFO 04-28 22:46:54 [monitor.py:33] torch.compile takes 36.96 s in total\n",
      "INFO 04-28 22:46:55 [kv_cache_utils.py:566] GPU KV cache size: 41,888 tokens\n",
      "INFO 04-28 22:46:55 [kv_cache_utils.py:569] Maximum concurrency for 251 tokens per request: 166.88x\n",
      "INFO 04-28 22:47:13 [gpu_model_runner.py:1534] Graph capturing finished in 17 secs, took 1.41 GiB\n",
      "INFO 04-28 22:47:13 [core.py:151] init engine (profile, create kv cache, warmup model) took 81.09 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10cb48f130e14ac0990a2f182ac2337c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Prompts:   0%|          | 0/19888 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.survival_runner import SurvivalRunner\n",
    "from src.generation.vanilla_model_vllm import VanillaGeneratorVLLM\n",
    "from src import utils\n",
    "from src.rating.detoxify import DetoxifyRater\n",
    "\n",
    "rater_backend = DetoxifyRater(model_type=\"unbiased\", amp=True)\n",
    "\n",
    "\n",
    "hf_key = utils.api_key_from_file(\"HF_KEY.txt\")\n",
    "generator_backend = VanillaGeneratorVLLM(\n",
    "    model_name=\"meta-llama/Llama-3.2-3B\",\n",
    "    hf_token=hf_key,\n",
    "    max_output_tokens=50,\n",
    "    # model_args={\"gpu_memory_utilization\": 0.95},\n",
    ")\n",
    "\n",
    "runner = SurvivalRunner(generator=generator_backend, rater=rater_backend, max_attempts=10)\n",
    "res = list(runner.generate(prompts=cal_prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data[0].ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data[0].ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatanate all rating arrays\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "all_ratings = []\n",
    "\n",
    "for res in tqdm(raw_data):\n",
    "    scores = [rt.scores[\"toxicity\"] for rt in res.ratings]\n",
    "    all_ratings.extend(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram of all ratings\n",
    "# show number of ratings in each bin\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "counts, edges, bars = plt.hist(all_ratings, bins=4)\n",
    "plt.bar_label(bars)\n",
    "plt.xlabel(\"Rating\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Histogram of Individual Ratings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets import PropDataset\n",
    "\n",
    "cal_props = PropDataset(\"data/rtp_500/split_1_0.5_0.1_0.2_0.2/cal.pkl\", score_name=\"toxicity\", threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_scores = [y[0] for (x, y) in tqdm(cal_props)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts, edges, bars = plt.hist(avg_scores, bins=4)\n",
    "plt.bar_label(bars)\n",
    "plt.xlabel(\"Rating\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Histogram of Average Ratings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted github.com/unitaryai/detoxify\n",
    "import transformers\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from src.loss import survival_loss, prop_loss\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import torchmetrics\n",
    "from scipy.stats import geom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_URL = \"https://github.com/unitaryai/detoxify/releases/download/\"\n",
    "MODEL_URLS = {\n",
    "    \"original\": DOWNLOAD_URL + \"v0.1-alpha/toxic_original-c1212f89.ckpt\",\n",
    "    \"unbiased\": DOWNLOAD_URL + \"v0.3-alpha/toxic_debiased-c7548aa0.ckpt\",\n",
    "    \"multilingual\": DOWNLOAD_URL + \"v0.4-alpha/multilingual_debiased-0b549669.ckpt\",\n",
    "    \"original-small\": DOWNLOAD_URL + \"v0.1.2/original-albert-0e1d6498.ckpt\",\n",
    "    \"unbiased-small\": DOWNLOAD_URL + \"v0.1.2/unbiased-albert-c8519128.ckpt\",\n",
    "}\n",
    "\n",
    "device = \"cpu\"\n",
    "model_type = \"original\"\n",
    "checkpoint_path = MODEL_URLS[model_type]\n",
    "loaded = torch.hub.load_state_dict_from_url(checkpoint_path, map_location=device)\n",
    "state_dict = loaded[\"state_dict\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.failure_model import ToxicClassifier\n",
    "import json\n",
    "\n",
    "\n",
    "config_path = \"/home/fre.gilad/source/llm-survival/configs/Prop_RTP_500_BERT.json\"\n",
    "config = json.load(open(config_path))\n",
    "\n",
    "model = ToxicClassifier(\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import utils\n",
    "\n",
    "hf_key = utils.api_key_from_file(\"HF_KEY.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "\n",
    "\n",
    "def weighted_mse(input: Tensor, target: Tensor, weight: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Computes the weighted mean squared error loss.\n",
    "    \"\"\"\n",
    "    return torch.mean(weight * (input - target) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from typing import Literal\n",
    "import math\n",
    "\n",
    "\n",
    "class ToxicClassifierProb(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        loss_type: Literal[\"bce\", \"scaled_bce\", \"log_mse\", \"quantile_mse\"],\n",
    "        hf_token: str,\n",
    "        optim_params=None,\n",
    "        max_survival_time: float = 1500,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        assert loss_type in [\"bce\", \"scaled_bce\", \"log_mse\", \"quantile_mse\"], f\"Invalid loss type: {loss_type}\"\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        if optim_params is None:\n",
    "            optim_params = {}\n",
    "\n",
    "        self.optim_params = optim_params\n",
    "        self.max_survival_time = max_survival_time\n",
    "        self.loss_type = loss_type\n",
    "\n",
    "        self.min_prob = 1 / self.max_survival_time\n",
    "        self.max_prob = 1 - 1 / self.max_survival_time\n",
    "\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            device_map=device,\n",
    "            token=hf_token,\n",
    "        )\n",
    "\n",
    "        self.model = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=1,\n",
    "            token=hf_token,\n",
    "        )\n",
    "\n",
    "        self.to(device)\n",
    "        self.train()\n",
    "\n",
    "        if self.loss_type == \"bce\":\n",
    "            self.loss_fn = self.bce_loss\n",
    "        elif self.loss_type == \"scaled_bce\":\n",
    "            self.loss_fn = self.scaled_bce_loss\n",
    "        elif self.loss_type == \"log_mse\":\n",
    "            self.loss_fn = self.log_mse_loss\n",
    "        elif self.loss_type == \"quantile_mse\":\n",
    "            self.loss_fn = self.quantile_mse_loss\n",
    "\n",
    "    def to_device(self, item):\n",
    "        if isinstance(item, torch.Tensor):\n",
    "            return item.to(self.device)\n",
    "        elif isinstance(item, dict):\n",
    "            return {k: self.to_device(v) for k, v in item.items()}\n",
    "        elif isinstance(item, (list, tuple)):\n",
    "            return [self.to_device(i) for i in item]\n",
    "\n",
    "        return item\n",
    "\n",
    "    def forward(self, prompts) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Predict probability directly\n",
    "        \"\"\"\n",
    "        inputs = self.tokenizer(prompts, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        inputs = inputs.to(self.device)\n",
    "        logits = self.model(**inputs)[0]\n",
    "\n",
    "        # predict p directly\n",
    "        return torch.sigmoid(logits).flatten()\n",
    "\n",
    "    def bce_loss(self, pred_probs: Tensor, meta: tuple[Tensor, ...]) -> Tensor:\n",
    "        \"\"\"\n",
    "        Computes the binary cross entropy loss between predicted and true survival probs.\n",
    "        \"\"\"\n",
    "        true_probs, _ = meta\n",
    "        true_probs = true_probs.to(pred_probs.dtype)\n",
    "        true_probs = torch.clamp(true_probs, min=self.min_prob, max=self.max_prob)\n",
    "        pred_probs = torch.clamp(pred_probs, min=self.min_prob, max=self.max_prob)\n",
    "        return F.binary_cross_entropy(pred_probs, true_probs)\n",
    "\n",
    "    def scaled_bce_loss(self, pred_probs: Tensor, meta: tuple[Tensor, ...]) -> Tensor:\n",
    "        true_probs, _ = meta\n",
    "        true_probs = true_probs.to(pred_probs.dtype)\n",
    "        true_probs = torch.clamp(true_probs, min=self.min_prob, max=self.max_prob)\n",
    "        pred_probs = torch.clamp(pred_probs, min=self.min_prob, max=self.max_prob)\n",
    "        return F.binary_cross_entropy(pred_probs, true_probs, weight=torch.sqrt(1 / true_probs))\n",
    "\n",
    "    def log_mse_loss(self, pred_probs: Tensor, meta: tuple[Tensor, ...]) -> Tensor:\n",
    "        true_probs, _ = meta\n",
    "        true_probs = true_probs.to(pred_probs.dtype)\n",
    "        true_probs = torch.clamp(true_probs, min=self.min_prob, max=self.max_prob)\n",
    "        pred_probs = torch.clamp(pred_probs, min=self.min_prob, max=self.max_prob)\n",
    "\n",
    "        true_logs = torch.log(true_probs / (1 - true_probs))\n",
    "        pred_logs = torch.log(pred_probs / (1 - pred_probs))\n",
    "        return F.mse_loss(pred_logs, true_logs)\n",
    "\n",
    "    def quantile_mse_loss(self, pred_probs: Tensor, meta: tuple[Tensor, ...], quantile: float = 0.5) -> Tensor:\n",
    "        true_probs, _ = meta\n",
    "        true_probs = true_probs.to(pred_probs.dtype)\n",
    "        true_probs = torch.clamp(true_probs, min=self.min_prob, max=self.max_prob)\n",
    "        pred_probs = torch.clamp(pred_probs, min=self.min_prob, max=self.max_prob)\n",
    "\n",
    "        true_quant = math.log(1 - quantile) / torch.log(1 - true_probs)\n",
    "        pred_quant = math.log(1 - quantile) / torch.log(1 - pred_probs)\n",
    "        return F.mse_loss(pred_quant, true_quant)\n",
    "\n",
    "    def training_step(self, batch, batch_idx) -> torch.Tensor:\n",
    "        prompts, meta = batch\n",
    "        pred_probs = self.forward(prompts)\n",
    "        meta = self.to_device(meta)\n",
    "        loss = self.loss_fn(pred_probs, meta)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx) -> None:\n",
    "        prompts, meta = batch\n",
    "        pred_probs = self.forward(prompts)\n",
    "\n",
    "        meta = self.to_device(meta)\n",
    "        bce_loss = self.bce_loss(pred_probs, meta)\n",
    "        scaled_bce_loss = self.scaled_bce_loss(pred_probs, meta)\n",
    "        log_mse_loss = self.log_mse_loss(pred_probs, meta)\n",
    "        quant_05_loss = self.quantile_mse_loss(pred_probs, meta, quantile=0.5)\n",
    "\n",
    "        self.log(\"val/bce_loss\", bce_loss, prog_bar=True)\n",
    "        self.log(\"val/scaled_bce_loss\", scaled_bce_loss, prog_bar=True)\n",
    "        self.log(\"val/log_mse_loss\", log_mse_loss, prog_bar=True)\n",
    "        self.log(\"val/quant_05_loss\", quant_05_loss, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), **self.optim_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets import PropDataset\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "model_name = \"answerdotai/ModernBERT-base\"\n",
    "\n",
    "for loss_type in [\"bce\", \"scaled_bce\", \"log_mse\", \"quantile_mse\"]:\n",
    "\n",
    "    model = ToxicClassifierProb(\n",
    "        model_name=model_name,\n",
    "        hf_token=hf_key,\n",
    "        loss_type=loss_type,\n",
    "        optim_params={\"lr\": 2e-5},\n",
    "        max_survival_time=500,\n",
    "    )\n",
    "\n",
    "    train_path = \"data/rtp_500/split_1_0.5_0.1_0.2_0.2/train.pkl\"\n",
    "    valid_path = \"data/rtp_500/split_1_0.5_0.1_0.2_0.2/val.pkl\"\n",
    "\n",
    "    ds_train = PropDataset(\n",
    "        train_path,\n",
    "        score_name=\"toxicity\",\n",
    "        threshold=0.5,\n",
    "    )\n",
    "\n",
    "    ds_valid = PropDataset(\n",
    "        valid_path,\n",
    "        score_name=\"toxicity\",\n",
    "        threshold=0.5,\n",
    "    )\n",
    "\n",
    "    batch_size = 128\n",
    "    num_workers = 0\n",
    "\n",
    "    dl_train = torch.utils.data.DataLoader(\n",
    "        ds_train,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    dl_valid = torch.utils.data.DataLoader(\n",
    "        ds_valid,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    max_epochs = 20\n",
    "    accumulate_grad_batches = 1\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        save_top_k=1,\n",
    "        verbose=True,\n",
    "        monitor=\"val/quant_05_loss\",\n",
    "        mode=\"min\",\n",
    "    )\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        devices=\"auto\",\n",
    "        max_epochs=max_epochs,\n",
    "        precision=32,\n",
    "        logger=True,\n",
    "        enable_checkpointing=True,\n",
    "        accumulate_grad_batches=accumulate_grad_batches,\n",
    "        callbacks=[checkpoint_callback],\n",
    "    )\n",
    "\n",
    "    trainer.fit(\n",
    "        model=model,\n",
    "        train_dataloaders=dl_train,\n",
    "        val_dataloaders=dl_valid,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
